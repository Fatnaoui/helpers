{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72a5a998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/fatnaoui/tools/miniforge3/envs/fasttext/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a22020b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers.filtering import looks_like_clean_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08fe2b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/fatnaoui/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "out_file = Path(\"en_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15b5af4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English Wikipedia dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading English Wikipedia dataset...\")\n",
    "dataset = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ba20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 2it [01:23, 41.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved 2 articles, 10590 words, and 455 sentences to: en_data.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_words = 3 # min words in each example\n",
    "n_sentences = 0\n",
    "n_articles = 0\n",
    "n_words = 0\n",
    "TARGET_ARTICLES = 10 # 20_000\n",
    "TARGET_SENTENCES = 50_000\n",
    "TARGET_WORDS = 10_000\n",
    "\n",
    "with out_file.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "    for example in tqdm(dataset, desc=\"Processing articles\"):\n",
    "\n",
    "        article_not_seen = True\n",
    "\n",
    "        # if n_articles >= TARGET_ARTICLES or n_sentences >= TARGET_SENTENCES or n_words >= TARGET_WORDS:\n",
    "        #     break\n",
    "\n",
    "        text = example[\"text\"]\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        for sent in sentences:\n",
    "            if n_sentences >= TARGET_SENTENCES:\n",
    "                break\n",
    "\n",
    "            sent = sent.strip()\n",
    "            if not sent:\n",
    "                continue\n",
    "\n",
    "            word_count = len(sent.split())\n",
    "            if word_count < min_words:\n",
    "                continue\n",
    "\n",
    "            # our extra \"robust English\" filter\n",
    "            if not looks_like_clean_english(sent):\n",
    "                continue\n",
    "\n",
    "            # normalize\n",
    "            sent = sent.lower()\n",
    "\n",
    "            f_out.write(sent.replace(\"\\n\", \" \") + \"\\n\")\n",
    "            n_words += word_count\n",
    "            n_sentences += 1\n",
    "            if article_not_seen:\n",
    "                n_articles += 1\n",
    "                article_not_seen = False\n",
    "\n",
    "print(f\"Done. Saved {n_articles} articles, {n_words} words, and {n_sentences} sentences to: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7efde4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasttext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
